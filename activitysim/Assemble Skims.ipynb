{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acquired-festival",
   "metadata": {},
   "source": [
    "# Assemble skims\n",
    "\n",
    "Put together all the skims from different places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import openmatrix as omx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-literacy",
   "metadata": {},
   "source": [
    "## Read original skims\n",
    "\n",
    "This already has walking and driving free-flow skims in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-processor",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_skims = omx.open_file('../data/skim_inputs/skims_wrong_names.omx', 'r')\n",
    "skims = omx.open_file('../model_inputs/skims.omx', 'w')\n",
    "skim_tract_mapping = pd.read_parquet('../la_abm/data/skim_tracts.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-plasma",
   "metadata": {},
   "outputs": [],
   "source": [
    "geoid_order = skim_tract_mapping.geoid.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-carpet",
   "metadata": {},
   "source": [
    "## Walking skims\n",
    "\n",
    "Just copy these over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# miles: https://github.com/BayAreaMetro/travel-model-one/blob/master/model-files/scripts/skims/HwySkims.job\n",
    "skims['DISTWALK'] = np.nan_to_num(np.array(orig_skims['walk_dist_km']) / 1.609, nan=-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these do seem to be network distances in the MTC data, not symmetrical\n",
    "skims['DIST'] = np.nan_to_num(np.array(orig_skims['car_distance_km']) / 1.609, nan=-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-coach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# workaround for https://github.com/ActivitySim/activitysim/issues/390\n",
    "skims.create_mapping('taz', np.arange(len(skim_tract_mapping)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-geology",
   "metadata": {},
   "source": [
    "## Driving skims\n",
    "\n",
    "A random forest model was used to create predictions for the ratio of congested travel time to uncongested travel time at each hour of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_res = pd.read_parquet('predicted_congestion_ratio.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-ethnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rf_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-dressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-foster",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_periods = {\n",
    "    'EA': (3, 5),\n",
    "    'AM': (5, 9),\n",
    "    'MD': (9, 14),\n",
    "    'PM': (14, 18),\n",
    "    'EV': (18, 24)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_flow_drive_time = pd.DataFrame(np.array(orig_skims['car_freeflow']), index=skim_tract_mapping.geoid.to_numpy(), columns=skim_tract_mapping.geoid.to_numpy()).stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, duration in time_periods.items():\n",
    "    start, end = duration\n",
    "    \n",
    "    print(name)\n",
    "    \n",
    "    # intentionally not including end, as it represents the hour after the time window is over\n",
    "    scale_factors = rf_res.loc[pd.IndexSlice[:,:,list(range(start, end))]].groupby(level=[0, 1]).congested_tt_ratio.mean()\n",
    "\n",
    "    congested_travel_time = free_flow_drive_time * scale_factors.reindex(free_flow_drive_time.index, fill_value=np.nan)\n",
    "    assert not congested_travel_time.isnull().any()\n",
    "    assert (congested_travel_time >= free_flow_drive_time).all()\n",
    "\n",
    "    # series back into matrix\n",
    "    congested_travel_time = congested_travel_time.unstack()\n",
    "    mtx = congested_travel_time.loc[skim_tract_mapping.geoid.to_numpy(), skim_tract_mapping.geoid.to_numpy()].to_numpy()\n",
    "    \n",
    "    # free flow drive times already in minutes\n",
    "    skims[f'SOV_TIME__{name}'] = mtx\n",
    "    skims[f'SOV_DIST__{name}'] = np.array(orig_skims['car_distance_km']) / 1.609  # miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free memory\n",
    "del free_flow_drive_time, scale_factors, congested_travel_time, rf_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-cooperation",
   "metadata": {},
   "source": [
    "### Non-SOV skims\n",
    "\n",
    "These are just computed as some fraction of the SOV trips - HOV2 and HOV3, and all toll trips, in the AM and PM periods over 5 miles are assumed to take 10% less time than SOV trips, or the free flow time, whichever is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-processor",
   "metadata": {},
   "outputs": [],
   "source": [
    "sov_dist = np.array(skims[f'SOV_DIST__AM']) # time invariant\n",
    "ff_time = np.array(orig_skims['car_freeflow'])\n",
    "for name in time_periods.keys():\n",
    "    sov_time = np.array(skims[f'SOV_TIME__{name}'])\n",
    "    if name in ('AM', 'PM'):\n",
    "        non_sov_time = np.copy(sov_time)\n",
    "        non_sov_time[sov_dist >= 5] *= 0.9\n",
    "        non_sov_time = np.maximum(non_sov_time, ff_time)\n",
    "    else:\n",
    "        non_sov_time = sov_time\n",
    "        \n",
    "    for non_sov in ('HOV2', 'HOV3', 'SOVTOLL', 'HOV2TOLL', 'HOV3TOLL'):\n",
    "        skims[f'{non_sov}_TIME__{name}'] = non_sov_time\n",
    "        skims[f'{non_sov}_DIST__{name}'] = sov_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-archives",
   "metadata": {},
   "source": [
    "### Tolls\n",
    "\n",
    "Assume bridge tolls are 0, and value tolls are 1.57/mile peak and 1.48 offpeak in 2021 dollars.\n",
    "\n",
    "CA 73 is [tolled for 14.6 miles](https://www.google.com/maps/dir/33.6547791,-117.8651856/33.5390436,-117.6747031/@33.6067673,-117.8551035,12.75z/data=!4m2!4m1!3e0?hl=en) and costs $1.57/mi peak and $1.48/mi offpeak. https://thetollroads.com/sites/default/files/FY21_RateCard.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpi_2000_2021 = 1.55\n",
    "peak_toll = 157 / cpi_2000_2021\n",
    "offpeak_toll = 148 / cpi_2000_2021\n",
    "\n",
    "for name in time_periods.keys():\n",
    "    if name in ('AM', 'PM'):\n",
    "        toll_rate = peak_toll\n",
    "    else:\n",
    "        toll_rate = offpeak_toll\n",
    "        \n",
    "    for toll_mode in ('SOVTOLL', 'HOV2TOLL', 'HOV3TOLL'):\n",
    "        # toll roads are rare in SoCal, assume 1/4 of trip is on tolled facility\n",
    "        skims[f'{toll_mode}_VTOLL__{name}'] = (sov_dist / 4) * toll_rate\n",
    "        # no toll bridges in SoCal, https://en.wikipedia.org/wiki/Vincent_Thomas_Bridge was the only one\n",
    "        skims[f'{toll_mode}_BTOLL__{name}'] = np.zeros_like(sov_dist)\n",
    "    \n",
    "    for mode in ('SOV', 'HOV2', 'HOV3'):\n",
    "        skims[f'{mode}_BTOLL__{name}'] = np.zeros_like(sov_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-small",
   "metadata": {},
   "source": [
    "## Transit skims\n",
    "\n",
    "Reformat these from the Julia output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-visit",
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds_to_hundredths_of_minutes = 100 / 60\n",
    "meters_to_hundredths_of_miles = 100 / 1609\n",
    "\n",
    "\n",
    "def generate_skim (df, col, idxs, missings, idx_shape):\n",
    "    out = df[col].iloc[idxs].to_numpy()\n",
    "    out[missings] = np.nan\n",
    "    return out.reshape(idx_shape)    \n",
    "    \n",
    "fares = {\n",
    "    'LOC': 175 / cpi_2000_2021,\n",
    "    'LRF': 175 / cpi_2000_2021,\n",
    "    'HVY': 175 / cpi_2000_2021,\n",
    "    'COM': 500 / cpi_2000_2021,\n",
    "    'EXP': 250 / cpi_2000_2021,\n",
    "    'TRN': 200 / cpi_2000_2021\n",
    "}\n",
    "    \n",
    "\n",
    "#for mode in ['LOC', 'LRF', 'HVY', 'COM', 'EXP', 'TRN']:\n",
    "for mode in ['LOC']:\n",
    "    for acc, egr in [\n",
    "        #('WLK', 'WLK'),\n",
    "        #('WLK', 'DRV'),\n",
    "        ('DRV', 'WLK')\n",
    "    ]:\n",
    "        print(f'{acc}_{mode}_{egr}')\n",
    "        tr_data = pd.read_parquet(f'../data/skim_inputs/transit_skims_{acc}_{mode}_{egr}.parquet')\n",
    "        tr_data = tr_data.set_index(['from_geoid', 'to_geoid'])\n",
    "        tr_data['time_period'] = tr_data.time_period.astype('category')\n",
    "        \n",
    "        for name in time_periods.keys():\n",
    "            tp_data = tr_data[tr_data.time_period == name].copy()\n",
    "            tp_data['intloc'] = np.arange(len(tp_data))\n",
    "            # unstack once and get a list of indices to avoid having to unstack once per column\n",
    "            idxs = tp_data.intloc.unstack(fill_value=-1).reindex(index=geoid_order, columns=geoid_order, fill_value=-1).to_numpy()\n",
    "            idx_shape = idxs.shape\n",
    "            idxs = idxs.reshape(-1)\n",
    "            missings = idxs == -1\n",
    "            # can't have nans in indexer array\n",
    "            idxs[missings] = 0\n",
    "            \n",
    "            skims[f'{acc}_{mode}_{egr}_WAIT__{name}'] = np.nan_to_num(generate_skim(tp_data, 'wait', idxs, missings, idx_shape) * seconds_to_hundredths_of_minutes, nan=-999)\n",
    "            totivt = np.nan_to_num(generate_skim(tp_data, 'total_ivt', idxs, missings, idx_shape) * seconds_to_hundredths_of_minutes, nan=-999)\n",
    "            keyivt = np.nan_to_num(generate_skim(tp_data, 'key_ivt', idxs, missings, idx_shape) * seconds_to_hundredths_of_minutes, nan=-999)\n",
    "            if mode != 'TRN':\n",
    "                skims[f'{acc}_{mode}_{egr}_KEYIVT__{name}'] = keyivt\n",
    "                skims[f'{acc}_{mode}_{egr}_TOTIVT__{name}'] = totivt\n",
    "            else:\n",
    "                skims[f'{acc}_{mode}_{egr}_IVT__{name}'] = totivt\n",
    "            skims[f'{acc}_{mode}_{egr}_WAUX__{name}'] = np.nan_to_num(generate_skim(tp_data, 'walk_time_xfers', idxs, missings, idx_shape) * seconds_to_hundredths_of_minutes, nan=-999)\n",
    "            skims[f'{acc}_{mode}_{egr}_IWAIT__{name}'] = np.nan_to_num(generate_skim(tp_data, 'initial_wait', idxs, missings, idx_shape) * seconds_to_hundredths_of_minutes, nan=-999)\n",
    "            skims[f'{acc}_{mode}_{egr}_XWAIT__{name}'] = np.nan_to_num(generate_skim(tp_data, 'wait_time_xfers', idxs, missings, idx_shape) * seconds_to_hundredths_of_minutes, nan=-999)\n",
    "            skims[f'{acc}_{mode}_{egr}_BOARDS__{name}'] = np.nan_to_num(generate_skim(tp_data, 'n_boardings', idxs, missings, idx_shape) * seconds_to_hundredths_of_minutes, nan=-999)\n",
    "            skims[f'{acc}_{mode}_{egr}_WACC__{name}'] = np.nan_to_num(generate_skim(tp_data, 'walk_access_time', idxs, missings, idx_shape) * seconds_to_hundredths_of_minutes, nan=-999)\n",
    "            skims[f'{acc}_{mode}_{egr}_WEGR__{name}'] = np.nan_to_num(generate_skim(tp_data, 'walk_egress_time', idxs, missings, idx_shape) * seconds_to_hundredths_of_minutes, nan=-999)\n",
    "            skims[f'{acc}_{mode}_{egr}_FAR__{name}'] = np.full_like(sov_time, fares[mode])\n",
    "\n",
    "            \n",
    "            if acc == 'DRV' or egr == 'DRV':\n",
    "                if 'drive_time' in tp_data.columns:\n",
    "                    skims[f'{acc}_{mode}_{egr}_DTIM__{name}'] = np.nan_to_num(generate_skim(tp_data, 'drive_time', idxs, missings, idx_shape) * seconds_to_hundredths_of_minutes, nan=-999)\n",
    "                    skims[f'{acc}_{mode}_{egr}_DDIST__{name}'] = np.nan_to_num(generate_skim(tp_data, 'drive_dist', idxs, missings, idx_shape) * meters_to_hundredths_of_miles, nan=-999)\n",
    "                else:\n",
    "                    print('WARN: USING ARBITRARY DRIVE TIMES AND DISTANCES!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "                    skims[f'{acc}_{mode}_{egr}_DTIM__{name}'] = np.full_like(sov_time, 1500)\n",
    "                    skims[f'{acc}_{mode}_{egr}_DDIST__{name}'] = np.full_like(sov_time, 500)\n",
    "            \n",
    "            if mode == 'LRF':\n",
    "                # no ferries in this model\n",
    "                skims[f'{acc}_{mode}_{egr}_FERRYIVT__{name}'] = np.full_like(sov_time, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-tourism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for skim in list(skims.list_matrices()):\n",
    "#     if 'DRV_LOC_WLK' in skim:\n",
    "#         skims.remove_node(f'/data/{skim}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-authentication",
   "metadata": {},
   "source": [
    "## Bike distances\n",
    "\n",
    "Assuming same as walk distance for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-wichita",
   "metadata": {},
   "outputs": [],
   "source": [
    "skims['DISTBIKE'] = np.array(skims['DISTWALK'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-energy",
   "metadata": {},
   "source": [
    "## Prune skims to only what is needed\n",
    "\n",
    "ActivitySim loads all skims into memory, so remove any skims that aren't in the SF model input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-relationship",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_skims = omx.open_file('../example_mtc_full/example_mtc_full/data/skims.omx')\n",
    "sf_matrices = set(sf_skims.list_matrices())\n",
    "sf_skims.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-accessory",
   "metadata": {},
   "outputs": [],
   "source": [
    "for skim in list(skims.list_matrices()):\n",
    "    if not skim in sf_matrices:\n",
    "        skims.remove_node(f'/data/{skim}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-color",
   "metadata": {},
   "source": [
    "## Intrazonal skims\n",
    "\n",
    "Just assume 0.5 * min(dist_to_other_zones) as was done in https://github.com/BayAreaMetro/travel-model-one/blob/master/model-files/scripts/skims/HwySkims.job\n",
    "\n",
    "This is also inordinately slow (1+ hr) for reasons I do not understand. This should take only a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-arabic",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mtxname in skims.list_matrices():\n",
    "    #if '_FAR__' not in mtxname:  # don't scale back fares\n",
    "    if 'DRV_LOC_WLK_' in mtxname and '_FAR__' not in mtxname:\n",
    "        mtx = np.array(skims[mtxname])\n",
    "        np.fill_diagonal(mtx, 1_000_000_000)  # make sure the diagonal is not the min\n",
    "        # nanmin should only return nan if all are nan\n",
    "        new_diag = np.nan_to_num(np.nanmin(np.where(mtx == -999, np.nan, mtx), axis=1) / 2, nan=-999)\n",
    "        np.fill_diagonal(mtx, new_diag)\n",
    "        skims.remove_node(f'/data/{mtxname}')  # no remove_matrix call?\n",
    "        skims[mtxname] = np.nan_to_num(mtx, -999)  # just for good measure, make sure all nans are banished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-marketplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(skims.list_matrices()) == len(sf_matrices), 'not all SF matrices present in LA data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "la_matrices = skims.list_matrices()\n",
    "[i for i in sf_matrices if i not in la_matrices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "skims.close()  # phew, all done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-august",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
