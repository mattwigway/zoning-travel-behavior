{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net present value and profitability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlalchemy as sq\n",
    "import geopandas as gp\n",
    "import census\n",
    "import os\n",
    "from shapely.geometry import Polygon\n",
    "from glob import glob\n",
    "import matplotlib.patches as mpatch\n",
    "import scipy.optimize\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "\n",
    "# NB remove when estimating discount rate\n",
    "discount_rate_ex = 0.06813142841167415\n",
    "\n",
    "capi = census.Census(os.environ['CENSUS_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('asu-light')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_percentile (vals, percentiles, weights):\n",
    "    if len(vals) != len(weights):\n",
    "        raise ArgumentError('values and weights arrays are not same length!')\n",
    "\n",
    "    nas = pd.isnull(vals) | pd.isnull(weights)\n",
    "\n",
    "    nnas = np.sum(nas)\n",
    "    if nnas > 0:\n",
    "        warn(f'found {nnas} NAs in data, dropping them')\n",
    "\n",
    "    vals = vals[~nas]\n",
    "    weights = weights[~nas]\n",
    "\n",
    "    weights = weights / np.sum(weights)\n",
    "    sortIdx = np.argsort(vals)\n",
    "    vals = vals.iloc[sortIdx]\n",
    "    weights = weights.iloc[sortIdx]\n",
    "\n",
    "    cumWeights = np.cumsum(weights)\n",
    "    if not isinstance(percentiles, np.ndarray):\n",
    "        percentiles = np.array(percentiles)\n",
    "    percentiles = percentiles / 100\n",
    "\n",
    "    # center weights, i.e. put the point value halfway through the weight\n",
    "    # https://github.com/nudomarinero/wquantiles/blob/master/wquantiles.py\n",
    "    centeredCumWeights = cumWeights - 0.5 * weights\n",
    "    return np.interp(percentiles, centeredCumWeights, vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = None\n",
    "ex = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT_RATE_NEW = 0.08 #0.065\n",
    "DISCOUNT_RATE_EX = 0.04125 #1.04 / 1.014 - 1\n",
    "#DISCOUNT_RATE_EX = DISCOUNT_RATE_NEW\n",
    "CAP_RATE = 0.04424375\n",
    "APPRECIATION = 0.014 # 0.014\n",
    "SCALE_FACTOR = 1.8031293436149882  # Scale factor to bring IPUMS rents in line with Zillow rental index\n",
    "VACANCY_RATE = 0.04  # 4% assumed vacancy\n",
    "TRANSACTION_COSTS = 0.09\n",
    "\n",
    "DB_URI = 'postgresql://matthewc@localhost:5432/matthewc'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT_RATE_EX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if new is None:\n",
    "    new = pd.read_sql('''\n",
    "        SELECT c.gid, c.clean_apn AS apn, c.county, c.puma, prototype, c.total_cost, c.total_rent, fit_sfh_duplex, fit_fourplex AS fit_threeplex, fit_sixplex\n",
    "            FROM diss.building_costs c\n",
    "            LEFT JOIN diss.building_fit f ON (c.gid = f.gid)\n",
    "            LEFT JOIN diss.gp16 p ON (p.gid = c.gid)\n",
    "            WHERE (p.total_rent IS NOT NULL OR p.vacant_npv IS NOT NULL) -- make sure existing rent was estimated\n",
    "            AND p.puma IS NOT NULL\n",
    "    ''', DB_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ex is None:\n",
    "    ex = pd.read_sql('''\n",
    "    WITH most_recent_sales AS (\n",
    "        SELECT \"Main_SalesPriceAmount\" AS price, \"Main_RecordingDate\" AS sale_date, \"PropertyInfo_ImportParcelID\" AS importparcelid,\n",
    "            ROW_NUMBER() OVER (PARTITION BY \"PropertyInfo_ImportParcelID\" ORDER BY \"Main_RecordingDate\" DESC) AS sale_number\n",
    "        FROM diss.ztrans t\n",
    "        WHERE SUBSTRING(t.\"Main_RecordingDate\", 1, 4) IN ('2013', '2014', '2015', '2016', '2017')\n",
    "    )\n",
    "    SELECT county, p.clean_apn AS apn,\n",
    "        p.gid, puma, tract, hqta, total_rent, vacant_npv, lu16, price, sale_date\n",
    "        FROM diss.gp16 p\n",
    "        LEFT JOIN most_recent_sales s ON (p.Main_ImportParcelId = s.importparcelid AND s.sale_number = 1)\n",
    "        WHERE p.scag_zn_co IN (\n",
    "            '1110', -- single family residential\n",
    "            '1111', -- high dens SF residential\n",
    "            '1112', -- med dens SF residential\n",
    "            '1113', -- low dens SF residential\n",
    "            '1150'  -- rural residential\n",
    "        )\n",
    "        AND p.Building_PropertyLandUseStndCode IN ('RR101', 'VL101')\n",
    "        AND (total_rent IS NOT NULL OR vacant_npv IS NOT NULL)\n",
    "        AND p.puma IS NOT NULL\n",
    "        ''', DB_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (len(new) / len(ex)) == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ex.total_rent * SCALE_FACTOR).describe().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_results (ex, new, discount_rate_ex, discount_rate_new, appreciation, cap_rate, op_cost, cost_scale, name):\n",
    "    if new is not None:\n",
    "        # see short-circuit code path used in calibration, where new is None\n",
    "        new['noi'] = new.total_rent * (1 - op_cost) * 12 * SCALE_FACTOR * (1 - VACANCY_RATE)  # annualize\n",
    "    ex['noi'] = ex.total_rent * (1 - op_cost) * 12 * SCALE_FACTOR * (1 - VACANCY_RATE)\n",
    "    \n",
    "    # since noi does not change year to year, p-v is just a multiplier of noi\n",
    "    ex_value = ex.vacant_npv.fillna(\n",
    "        ex.noi * sum([1 / ((1 + discount_rate_ex)**i * (1 + appreciation) ** i) for i in range(10)])  # rental value\n",
    "        + ex.noi / cap_rate / (1 + discount_rate_ex) ** 10 * (1 + appreciation) ** 10 * (1 - TRANSACTION_COSTS) # ultimate sale value\n",
    "    )\n",
    "    \n",
    "    if new is None:\n",
    "        # short-circuit code path used in calibration\n",
    "        return ex_value\n",
    "    else:\n",
    "        ex['value'] = ex_value\n",
    "        \n",
    "    new['value'] = (\n",
    "        new.noi * sum([1 / ((1 + discount_rate_new)**i ) for i in range(2, 10)])  # rental value\n",
    "        + new.noi / cap_rate / (1 + discount_rate_new) ** 10 * (1 + appreciation) ** 10 * (1 - TRANSACTION_COSTS)  # ultimate sale value\n",
    "        - new.total_cost * cost_scale / 2  # Construction cost, year one\n",
    "        - new.total_cost * cost_scale / 2 / (1 + discount_rate_new)  # Construction cost, year 2\n",
    "    )\n",
    "    \n",
    "    print(ex.value.describe().round())\n",
    "    median_ex = np.percentile(ex.value, 50)\n",
    "    \n",
    "    # account for the fit\n",
    "    new.loc[new.prototype.isin(['sfh', 'duplex']) & ~new.fit_sfh_duplex, 'value'] = -np.inf\n",
    "    new.loc[new.prototype.isin(['threeplex']) & ~new.fit_threeplex, 'value'] = -np.inf\n",
    "    new.loc[new.prototype.isin(['sixplex']) & ~new.fit_sixplex, 'value'] = -np.inf\n",
    "        \n",
    "    values = (v := new.set_index(['gid', 'prototype'])).value.unstack()\n",
    "    values.head().round()\n",
    "    \n",
    "    values = (\n",
    "        values.merge(ex[['gid', 'value', 'hqta']]\n",
    "                     .rename(columns={'value': 'existing'}), left_index=True, right_on='gid', how='left', validate='1:1')\n",
    "                    .set_index('gid')\n",
    "    )\n",
    "    assert not values.existing.isnull().any()\n",
    "    \n",
    "    values.to_parquet(f'../data/{name}_net_present_value.parquet')\n",
    "    \n",
    "    hqta_values = values.copy()\n",
    "    hqta_values.loc[~hqta_values.hqta, ['sfh', 'duplex', 'threeplex', 'sixplex']] = -np.inf\n",
    "    hqta_values.to_parquet(f'../data/{name}_hqta_net_present_value.parquet')\n",
    "    \n",
    "    most_profitable = (\n",
    "        values[['existing', 'sfh', 'duplex', 'threeplex', 'sixplex']]\n",
    "        .apply(lambda r: r.idxmax(), 1).astype('category').cat.set_categories(['existing', 'sfh', 'duplex', 'threeplex', 'sixplex'])\n",
    "    )\n",
    "    \n",
    "    counts = pd.DataFrame({'Total': most_profitable.value_counts(), 'Percent': (most_profitable.value_counts(normalize=True) * 100).round(1).astype('str') + '%'}).transpose()[['existing', 'duplex', 'threeplex', 'sixplex']]\\\n",
    "        .rename(columns={'existing': 'No change', 'sfh': 'New single-family', 'threeplex': 'Threeplex', 'duplex': 'Duplex', 'sixplex': 'Sixplex'})\n",
    "    total_parcels = counts.loc['Total', :].sum()\n",
    "    counts['Total'] = 'test'\n",
    "    counts.loc['Total', 'Total'] = total_parcels\n",
    "    counts.loc['Percent', 'Total'] = '100.0%'\n",
    "    \n",
    "    ex = ex.set_index('gid')\n",
    "    ex['most_profitable'] = most_profitable.reindex(ex.index)    \n",
    "    ex['not_existing'] = ex.most_profitable != 'existing'\n",
    "    locations = ex.groupby('tract').not_existing.mean() * 100\n",
    "    \n",
    "    v = most_profitable.value_counts()\n",
    "    total = v.duplex + v.threeplex * 2 + v.sixplex * 5 # duplex has one marginal unit etc\n",
    "    # add vacant lots\n",
    "    total += (~ex.vacant_npv.isnull() & ex.not_existing).sum()  # 1 add'l unit for vacant lots\n",
    "    print(f'total units: {total}')\n",
    "    \n",
    "    display(counts)\n",
    "    \n",
    "    hqta_profitable = ex.loc[ex.hqta, 'most_profitable']\n",
    "    hqta_counts = pd.DataFrame({'Total': hqta_profitable.value_counts(), 'Percent': (hqta_profitable.value_counts(normalize=True) * 100).round(1).astype('str') + '%'}).transpose()[['existing', 'duplex', 'threeplex', 'sixplex']]\\\n",
    "        .rename(columns={'existing': 'No change', 'threeplex': 'Threeplex', 'duplex': 'Duplex', 'sixplex': 'Sixplex'})\n",
    "    total_units = hqta_counts.loc['Total', :].sum()\n",
    "    hqta_counts['Total'] = 'test'\n",
    "    hqta_counts.loc['Total', 'Total'] = total_units\n",
    "    hqta_counts.loc['Percent', 'Total'] = '100.0%'\n",
    "    v = hqta_profitable.value_counts()\n",
    "    hqta_total = v.duplex + v.threeplex * 2 + v.sixplex * 5 # duplex has one marginal unit etc\n",
    "    # add vacant lots\n",
    "    hqta_total += (ex.not_existing & ex.hqta & ~ex.vacant_npv.isnull()).sum()  # 1 addl unit for vacant lots\n",
    "    \n",
    "    return counts, hqta_counts, locations, total, hqta_total, median_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = {\n",
    "    'Current appreciation':  {\n",
    "        'discount_rate_new': 0.11,\n",
    "        'discount_rate_ex': 0.06,\n",
    "        'cap_rate': CAP_RATE,\n",
    "        'appreciation': 0.0480328437009123,\n",
    "        'op_cost': 0.45,\n",
    "        'cost_scale': 1.1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrate the single-family discount rate\n",
    "\n",
    "The cap rate and appreciation rate are estimated from data, while the discount rate is just made up, and a wide range of values are plausible. Calibrate the discount rate so that the median home is estimated exactly correctly based on past sale values - i.e. 50% of homes have their values underestimated, and 50% have them overestimated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_with_sales = ex.loc[~ex.price.isnull() & (ex.price > 5000)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all sale values in 2017 dollars\n",
    "# apply inflation using Zillow Home Value Index for LA, SFR only\n",
    "# https://www.zillow.com/research/data/\n",
    "zhvi = pd.read_csv('../data/Metro_zhvi_uc_sfr_tier_0.33_0.67_sm_sa_mon.csv')\n",
    "zhvi_la = zhvi.loc[zhvi.RegionName == 'Los Angeles-Long Beach-Anaheim, CA', [i for i in zhvi.columns if i.startswith('20')]].iloc[0]\n",
    "# scale to appropriate scale factors\n",
    "zhvi_la /= zhvi_la.loc['2017-12-31'] # this is the end of the ZTrans data and also the end of the PUMS data used to estimate rents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_with_sales['date'] = pd.to_datetime(ex_with_sales.sale_date)\n",
    "\n",
    "# MonthEnd(0) - end of this month, even if this is the end of the month.\n",
    "ex_with_sales['sale_month'] = (ex_with_sales.date + MonthEnd(0)).astype(str)\n",
    "\n",
    "ex_with_sales.sale_month.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_with_sales['zhvi'] = zhvi_la.loc[ex_with_sales.sale_month].values\n",
    "assert not ex_with_sales.zhvi.isnull().any()\n",
    "ex_with_sales['sale_price17'] = ex_with_sales.price / ex_with_sales.zhvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_median_npv_to_sale_ratio (discount_rate):\n",
    "    scenario = {**scenarios['Current appreciation'], 'discount_rate_ex': discount_rate}\n",
    "    npvs = compute_results(ex_with_sales, None, name=None, **scenario).reindex(ex_with_sales.index)\n",
    "    return np.log(np.percentile(npvs / ex_with_sales.sale_price17, 50)) # log makes root at zero, and also (I think) linearizes somehow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_root = scipy.optimize.root_scalar(log_median_npv_to_sale_ratio, x0=0.05, x1=0.06, method='secant', xtol=1e-5)\n",
    "discount_rate_ex = discount_root.root\n",
    "assert discount_root.converged\n",
    "discount_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update tge current appreciation scenario with the discount rate we found\n",
    "scenarios['Current appreciation']['discount_rate_ex'] = discount_rate_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_with_sales['current_npv'] = current_npvs = compute_results(ex_with_sales, None, name=None, **scenarios['Current appreciation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_data = ex_with_sales[ex_with_sales.sale_price17 <= 2e6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = 1 - np.sum((r2_data.current_npv - r2_data.sale_price17) ** 2) / np.sum((r2_data.sale_price17 - r2_data.sale_price17.mean()) ** 2)\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(9, 6))\n",
    "plt.scatter(ex_with_sales.sale_price17, ex_with_sales.current_npv, s=0.0001, color='black')\n",
    "plt.plot([0, 5e6], [0, 5e6], color='C5', lw=2)\n",
    "plt.plot([0, 5e6], [0, 5e6], color='C0', lw=0.75)\n",
    "plt.xlim(0, 2e6)\n",
    "plt.ylim(0, 1.5e6)\n",
    "plt.xticks([0, 5e5, 1e6, 1.5e6, 2e6], ['$0', '$500k', '$1m', '$1.5m', '$2m'])\n",
    "plt.yticks([0, 5e5, 1e6, 1.5e6], ['$0', '$500k', '$1m', '$1.5m'])\n",
    "plt.xlabel('Sale price (2017 dollars)')\n",
    "plt.ylabel('Predicted net present value')\n",
    "plt.savefig('../../dissertation/fig/construction/npv_vs_sale.png', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run for multiple scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios['Base'] = {\n",
    "    'discount_rate_new': 0.11,\n",
    "    'discount_rate_ex': discount_rate_ex,\n",
    "    'cap_rate': CAP_RATE,\n",
    "    'appreciation': 0.014,  # CPI\n",
    "    'op_cost': 0.45,\n",
    "    'cost_scale': 1.1 # 10% contingency\n",
    "}\n",
    "\n",
    "scenarios['Low discount rate'] = {\n",
    "    'discount_rate_new': 0.08,\n",
    "    'discount_rate_ex': 0.04125,\n",
    "    'cap_rate': CAP_RATE,\n",
    "    'appreciation': 0.014,  # CPI\n",
    "    'op_cost': 0.45,\n",
    "    'cost_scale': 1.1 # 10% contingency\n",
    "}\n",
    "\n",
    "scenarios['Equal discount rate (8% existing and new)'] = {\n",
    "    'discount_rate_new': 0.08,\n",
    "    'discount_rate_ex': 0.08,\n",
    "    'cap_rate': CAP_RATE,\n",
    "    'appreciation': 0.014,  # CPI\n",
    "    'op_cost': 0.45,\n",
    "    'cost_scale': 1.1 # 10% contingency\n",
    "}\n",
    "\n",
    "scenarios['Low operating cost (25%)'] = {\n",
    "    'discount_rate_new': 0.11,\n",
    "    'discount_rate_ex': discount_rate_ex,\n",
    "    'cap_rate': CAP_RATE,\n",
    "    'appreciation': 0.0480328437009123,\n",
    "    'op_cost': 0.25,\n",
    "    'cost_scale': 1.1 # 10% contingency\n",
    "}\n",
    "\n",
    "scenarios['High construction cost'] = {\n",
    "    'discount_rate_new': 0.11,\n",
    "    'discount_rate_ex': discount_rate_ex,\n",
    "    'cap_rate': CAP_RATE,\n",
    "    'appreciation': 0.014,  # CPI\n",
    "    'op_cost': 0.45,\n",
    "    'cost_scale': 1.4 # 30% on top\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = {}\n",
    "for name, srio in scenarios.items():\n",
    "    print(name)\n",
    "    out[name] = dict(zip(('counts', 'hqta_counts', 'locations', 'total_units',  'hqta_total_counts', 'median_val_ex'), compute_results(ex.copy(), new.copy(), name=name, **srio)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump scenarios to a handy-dandy table\n",
    "scenario_table = (\n",
    "    (pd.DataFrame(scenarios).transpose() * 100).apply(lambda c: c.round(3).astype('str') + '%')\n",
    "        .rename(columns={\n",
    "            'cap_rate': 'Capitalization rate',\n",
    "            'appreciation': 'Appreciation rate',\n",
    "            'op_cost': 'Operating cost'\n",
    "        }))\n",
    "print(scenario_table.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLD CODE BELOW HERE.\n",
    "## Sensitivity test table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "\n",
    "pct = '{:.2f}%'.format\n",
    "\n",
    "for sname, scenario in scenarios.items():\n",
    "    output.append(np.array([[f'\\\\centering\\\\textbf{{{sname}}}', '', '', '', '', '', '']]))\n",
    "    output.append(np.array([\n",
    "        [f'\\\\textit{{{i}}}' for i in\n",
    "            ['Marginal units', 'Discount rate (new construction)', 'Discount rate (existing structure)', 'Capitalization rate', 'Appreciation rate', 'Operating cost', '']\n",
    "        ],\n",
    "        [\n",
    "            '{:,d}'.format(out[sname]['total_units']),\n",
    "            pct(scenario['discount_rate_new'] * 100),\n",
    "            pct(scenario['discount_rate_ex'] * 100),\n",
    "            pct(scenario['cap_rate'] * 100),\n",
    "            pct(scenario['appreciation'] * 100),\n",
    "            pct(scenario['op_cost']),\n",
    "            ''\n",
    "        ]\n",
    "    ]))\n",
    "    cts = out[sname]['counts'].copy()\n",
    "    cts.loc['Total', :] = cts.loc['Total', :].apply('{:,d}'.format)\n",
    "    cts = cts.reset_index()\n",
    "    output.append(np.array([[f'\\\\textit{{{i}}}' if i != 'index' else '' for i in cts.columns]]))\n",
    "    output.append(cts.to_numpy())\n",
    "\n",
    "res = pd.DataFrame(np.concatenate(output))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.to_latex(escape=False, index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HQTA scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hqta_profitable = ex.loc[ex.hqta, 'most_profitable']\n",
    "counts = pd.DataFrame({'Total': hqta_profitable.value_counts(), 'Percent': (hqta_profitable.value_counts(normalize=True) * 100).round(1).astype('str') + '%'}).transpose()[['existing', 'duplex', 'threeplex', 'sixplex']]\\\n",
    "    .rename(columns={'existing': 'No change', 'threeplex': 'Threeplex', 'duplex': 'Duplex', 'sixplex': 'Sixplex'})\n",
    "counts.insert(1, 'New single-family', [0, '0.0%'])\n",
    "total_units = counts.loc['Total', :].sum()\n",
    "counts['Total'] = 'test'\n",
    "counts.loc['Total', 'Total'] = total_units\n",
    "counts.loc['Percent', 'Total'] = '100.0%'\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.Duplex + counts.Threeplex * 2 + counts.Sixplex * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.Duplex + counts.Threeplex + counts.Sixplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "219884 / 545130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://censusreporter.org/profiles/05000US06037-los-angeles-county-ca/\n",
    "EXISTING_UNITS = 3_579_423\n",
    "219884 / EXISTING_UNITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gentrification???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_poverty = pd.DataFrame(capi.acs5.state_county_tract(['B17001_001E', 'B17001_002E', 'B25001_001E', 'B03002_001E', 'B03002_003E'], '06', '037', census.ALL, year=2017))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_poverty['poverty_rate'] = pct_poverty.B17001_002E / pct_poverty.B17001_001E * 100\n",
    "pct_poverty['pct_white'] = pct_poverty.B03002_003E / pct_poverty.B03002_001E * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_poverty['GEOID'] = pct_poverty.state.str.cat(pct_poverty.county).str.cat(pct_poverty.tract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devpoverty = ex.merge(pct_poverty, left_on='tract', right_on='GEOID', how='left', validate='m:1')\n",
    "#assert not devpoverty.poverty_rate.isnull().any()  # TODO missing 5 tracts\n",
    "with pd.option_context('display.float_format', '{:.1f}%'.format):\n",
    "    vals = pd.concat([\n",
    "        devpoverty[devpoverty.not_existing][['poverty_rate', 'pct_white']].quantile([0.05, 0.25, 0.50, 0.75, 0.95]),\n",
    "        devpoverty[['poverty_rate', 'pct_white']].quantile([0.05, 0.25, 0.50, 0.75, 0.95])\n",
    "    ],\n",
    "        keys=['Redeveloped parcels', 'All parcels'],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    vals.index = ['5th percentile', '25th percentile', 'Median', '75th percentile', '95th percentile']\n",
    "    \n",
    "    display(vals)\n",
    "    print(vals.to_latex())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to overall housing stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devpoverty[devpoverty.poverty_rate.isnull()].tract_x.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(pd.DataFrame(capi.acs5.tables()).set_index('name').loc['B03002'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(capi.acs5.tables()).set_index('name').loc['B02001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
