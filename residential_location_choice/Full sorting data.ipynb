{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract data for full Los Angeles simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy as sq\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gp\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "import factor_analyzer\n",
    "\n",
    "CAP_RATE = 0.04424375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('asu-light')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to database\n",
    "conn = sq.create_engine('sqlite:////Volumes/Pheasant Ridge/IPUMS/scag_sorting_5yr_abm/scag_sorting_5yr_abm.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipums = pd.read_sql('''SELECT\n",
    "serial,\n",
    "year,\n",
    "numprec,\n",
    "hhwt,\n",
    "puma,\n",
    "ownershp,\n",
    "rentgrs,\n",
    "hhincome,\n",
    "builtyr2,\n",
    "unitsstr,\n",
    "rooms,\n",
    "bedrooms,\n",
    "vehicles,\n",
    "multgen,\n",
    "educ,\n",
    "empstat,\n",
    "indnaics,\n",
    "bpl,\n",
    "valueh,\n",
    "race,\n",
    "hispan,\n",
    "age,\n",
    "pwpuma00 AS workpuma,\n",
    "pwstate2 AS workstate,\n",
    "pwcounty AS workcounty,\n",
    "tranwork\n",
    "FROM ipums\n",
    "''', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipums.ownershp.value_counts() # what does N/A mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipums = ipums[ipums.ownershp != 'N/A'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Person-level variables\n",
    "\n",
    "Summarize person-level variables to household level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipums['age'] = ipums.age.replace({'Less than 1 year old': 0, '90 (90+ in 1980 and 1990)': 90}).astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipums['university'] = ipums.educ.isin(['4 years of college', '5+ years of college'])\n",
    "ipums['worker'] = ipums.empstat == 'Employed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipums['immigrant'] = ~ipums.bpl.isin(['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut',\n",
    "                                           'Delaware', 'District of Columbia', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana',\n",
    "                                           'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan',\n",
    "                                           'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey',\n",
    "                                           'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania',\n",
    "                                           'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia',\n",
    "                                           'Washington', 'West Virginia', 'Wisconsin', 'Wyoming', 'U.S. Virgin Islands', 'Atlantic Islands', 'Guam',\n",
    "                                          'Puerto Rico', 'Pacific Islands', 'American Samoa', 'Other US Possessions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipums.bpl[ipums.immigrant].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipums.race.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoid creating multiindex below when summing and doing an all by just creating two columns\n",
    "ipums['nonhispwhite_count'] = ipums['nonhispwhite_all'] = (ipums.hispan == 'Not Hispanic') & (ipums.race == 'White')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipums['child'] = ipums.age < 18\n",
    "ipums['senior'] = ipums.age >= 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpd = ipums.groupby('serial')\n",
    "hhs = pd.DataFrame({\n",
    "    'university': grpd.university.any(),\n",
    "    'worker': grpd.worker.sum(),\n",
    "    'immigrant': grpd.immigrant.any(),\n",
    "    'child': grpd.child.any(),\n",
    "    'senior': grpd.senior.any(),\n",
    "    'nonhispwhite_count': grpd.nonhispwhite_count.sum(),\n",
    "    'nonhispwhite_all': grpd.nonhispwhite_all.all(),\n",
    "    'year': grpd.year.first(),\n",
    "    'numprec': grpd.numprec.first(),\n",
    "    'hhwt': grpd.hhwt.first(),\n",
    "    'puma': grpd.puma.first(),\n",
    "    'ownershp': grpd.ownershp.first(),\n",
    "    'rentgrs': grpd.rentgrs.first(),\n",
    "    'valueh': grpd.valueh.first(),\n",
    "    'hhincome': grpd.hhincome.first(),\n",
    "    'builtyr2': grpd.builtyr2.first(),\n",
    "    'unitsstr': grpd.unitsstr.first(),\n",
    "    'rooms': grpd.rooms.first(),\n",
    "    'bedrooms': grpd.bedrooms.first(),\n",
    "    'vehicles': grpd.vehicles.first(),\n",
    "    'multgen': grpd.multgen.first()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove mobile homes, other\n",
    "hhs = hhs[~hhs.unitsstr.isin(['Mobile home or trailer',\n",
    "       'Boat, tent, van, other'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs['sfmf'] = hhs.unitsstr.map({\n",
    "    '1-family house, attached': 'SF',\n",
    "     '1-family house, detached': 'SF',\n",
    "     '10-19 family building': 'MF',\n",
    "     '2-family building': 'MF',\n",
    "     '20-49 family building': 'MF',\n",
    "     '3-4 family building': 'MF',\n",
    "     '5-9 family building': 'MF',\n",
    "     '50+ family building': 'MF'\n",
    "}).astype('category')\n",
    "assert not hhs.sfmf.isnull().any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs.unitsstr.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hhs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make rent and value comparable\n",
    "\n",
    "Eventually these will be estimated separately, but for now just put them together using the cap rate previously estimated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs['annvalue'] = np.where(hhs.ownershp == 'Rented', hhs.rentgrs * 12, hhs.valueh * CAP_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs['annvalue'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakpoints for new/med/old\n",
    "\n",
    "When does a property lose its 'new' value? Estimate a simple hedonic to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs['builtyr2'] = hhs.builtyr2.astype('category')\n",
    "hhs.builtyr2.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs['builtyrmid'] = hhs.builtyr2.map({\n",
    "    '1939 or earlier': 1939,\n",
    "    '1940-1949': 1945, \n",
    "    '1950-1959': 1955,\n",
    "    '1960-1969': 1965,\n",
    "    '1970-1979': 1975,\n",
    "    '1980-1989': 1985,\n",
    "    '1990-1994 (1990-1999 in the 2005-onward ACS and the PRCS)': 1995,\n",
    "    '2000-2004 (1999-2002 in the 2000-2002 ACS)': 2002,\n",
    "    '2005 (2005 or later in datasets containing 2005, 2006, or 2007 ACS/PRCS data)': 2005,\n",
    "    '2006': 2006, '2007': 2007, '2008': 2008, '2009': 2009, '2010': 2010, '2011': 2011, '2012': 2012, '2013': 2013, '2014': 2014,\n",
    "       '2015': 2015, '2016': 2016, '2017': 2017}).astype('int64')\n",
    "hhs['appxage'] = pd.cut(hhs.year.astype('int64') - hhs.builtyrmid, [-1, 5, 10, 15, 30, 40, 50, 60, 70, 100_000])\n",
    "assert not hhs.appxage.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(hhs[hhs.ownershp == 'Rented'].annvalue, alpha=0.15, color='red', bins=20)\n",
    "plt.hist(hhs[hhs.ownershp != 'Rented'].annvalue, alpha=0.15, color='blue', bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs[hhs.ownershp == 'Rented'].annvalue.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs[hhs.ownershp != 'Rented'].annvalue.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs.appxage.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exog = pd.get_dummies(hhs[[\n",
    "    'appxage',\n",
    "    'bedrooms',\n",
    "    'ownershp',\n",
    "    'sfmf',\n",
    "    'puma'\n",
    "]]).drop(columns=['appxage_(70, 100000]', 'bedrooms_No bedrooms', 'sfmf_MF', 'ownershp_Rented', 'puma_03701'])\n",
    "hedonic = sm.OLS(hhs.annvalue, sm.add_constant(exog))\n",
    "hedonic_fit = hedonic.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs.puma.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check vifs\n",
    "#vifs = pd.Series([variance_inflation_factor(hedonic.exog, i) for i in range(len(hedonic.exog_names))], index=hedonic.exog_names)\n",
    "#vifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.option_context('display.max_rows', None):\n",
    "#     display(vifs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hedonic_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [f'appxage_{i}' for i in hhs.appxage.cat.categories][:-1]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(np.arange(len(params) + 1), [*hedonic_fit.params[params], 0], yerr=[*(hedonic_fit.bse[params] * 1.96), 0])\n",
    "plt.xticks(np.arange(len(params) + 1), [f'{i.left + 1}â€“{i.right}' if i.right < 10000 else '71 or more' for i in hhs.appxage.cat.categories], rotation='vertical')\n",
    "plt.xlabel('Age of structure (approximate)')\n",
    "plt.ylabel('Cost premium (dollars, relative to more than 70 years)')\n",
    "plt.savefig('../../dissertation/fig/sorting/value_age.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs.appxage.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs['agecat'] = pd.cut(hhs.year.astype('int64') - hhs.builtyrmid, [-1, 15, np.inf], labels=['new', 'old'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(hhs.builtyr2, hhs.agecat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs['ownrent'] = hhs.ownershp.map({\n",
    "    'Owned or being bought (loan)': 'own',\n",
    "    'Rented': 'rent'\n",
    "})\n",
    "assert not hhs.ownrent.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs['choice'] = hhs.puma.str.cat(hhs.sfmf, sep='_').str.cat(hhs.agecat, sep='_').str.cat(hhs.ownrent, sep='_')\n",
    "len(hhs.choice.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs.choice.value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hhs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighborhood attributes\n",
    "\n",
    "Add some neighborhood attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### School quality\n",
    "\n",
    "2012 STAR tests (...ah that brings back memories): https://star.cde.ca.gov/star2012/ResearchFileList.aspx?rf=True&ps=True\n",
    "\n",
    "Spatial data: http://www.californiaschoolcampusdatabase.org/#download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testResults = pd.read_csv('../data/schoolquality/ca2012_1_csv_v3.txt')\n",
    "entities = pd.read_csv('../data/schoolquality/ca2012entities_csv.txt')\n",
    "entities = entities[entities['Type Id'] == 7].copy() # only public, not charter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schools = gp.read_file('../data/schoolquality/CSCD_school_centroids.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities['CDSCode'] = entities['County Code'].astype('str').str.zfill(2) +\\\n",
    "    entities['District Code'].astype('str').str.zfill(5) +\\\n",
    "    entities['School Code'].astype('str').str.zfill(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with spatial data\n",
    "# Note that some schools have more than one physical location, so are represented more than once in the output\n",
    "schools = schools.merge(entities, on='CDSCode', how='inner', validate='m:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testResults['CDSCode'] = testResults['County Code'].astype('str').str.zfill(2) +\\\n",
    "    testResults['District Code'].astype('str').str.zfill(5) +\\\n",
    "    testResults['School Code'].astype('str').str.zfill(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use fifth grade math because it's always done in elementary school (the most local school type)\n",
    "# and everyone takes it pretty much\n",
    "fifthGradeMath = testResults.loc[(testResults['Test Id'] == 8) & (testResults.Grade == 5)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schools = schools.merge(fifthGradeMath, on='CDSCode', how='inner', validate='m:1')\n",
    "schools = schools[schools['Percentage At Or Above Proficient'] != '*'].copy() # drop schools with missing data (usually less than ten students took test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schools['Percentage At Or Above Proficient'] = schools['Percentage At Or Above Proficient'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(schools['Percentage At Or Above Proficient'], bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate to PUMAs\n",
    "pumas = gp.read_file('/Volumes/Pheasant Ridge/IPUMS/pumas/ipums_puma_2010.shp')\n",
    "pumas = pumas[pumas.STATEFIP == '06'].to_crs(epsg=4326).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schools = schools.to_crs(epsg=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schools = gp.sjoin(schools, pumas, op='within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumaScores = schools.groupby('PUMA')['Percentage At Or Above Proficient'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumas['score'] = pumaScores.loc[pumas.PUMA].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumas.plot(column='score', legend=True)\n",
    "plt.xlim(-119, -117)\n",
    "plt.ylim(33, 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs['fifthGradeMathMedianProficient'] = pumaScores.loc[hhs.puma].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs['bedrooms'] = hhs.bedrooms.map({\n",
    "    '3': 3, '2': 2, '1': 1, 'No bedrooms': 0, '4 (1970-2000, 2000-2007 ACS/PRCS)': 4,\n",
    "       '5+ (1970-2000, 2000-2007 ACS/PRCS)': 5, '10': 10, '7': 7, '9': 9\n",
    "}).astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from my new residents transit project\n",
    "pumaSld = pd.read_csv('../../../new_residents_transit/data/sld_2010.csv', dtype={'stpuma': 'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumaSld['state'] = pumaSld.stpuma.str.slice(0, 2)\n",
    "pumaSld['puma'] = pumaSld.stpuma.str.slice(2)\n",
    "pumaSld = pumaSld[pumaSld.state == '06'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs = hhs.reset_index().merge(pumaSld[['jobAccessAuto', 'intersectionDens', 'puma']], on='puma', how='left', validate='m:1')\\\n",
    "    .set_index('serial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not hhs.choice.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs['numprec'] = hhs.numprec.replace({\n",
    "    '1 person record': 1\n",
    "}).astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transit access\n",
    "\n",
    "From University of Minnesota Accessibility Observatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracc = pd.read_csv('../data/ao/transitaccess.csv', dtype={'geoid': 'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracc = tracc.dropna(subset=['threshold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MABLE block to puma mapping, weighted by housing units\n",
    "blockpuma = pd.read_csv('../data/mable_puma_block_hu.csv', skiprows=[1], dtype={'state': 'str', 'county': 'str', 'puma12': 'str', 'tract': 'str', 'block': 'str'})\n",
    "blockpuma['blockgeoid'] = blockpuma.county.str.cat(blockpuma.tract.str.replace('.', '')).str.cat(blockpuma.block)\n",
    "blockpuma.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracc = tracc.merge(blockpuma, left_on='geoid', right_on='blockgeoid', how='inner', validate='1:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize to PUMAs weighting by housing units in block in 2010\n",
    "pumaTracc = pd.DataFrame(tracc.groupby('puma12').apply(lambda df: np.average(df.jobs, weights=df.hus10)).rename('jobacc30'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumaTracc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs = hhs.merge(pumaTracc, left_on='puma', right_index=True, how='left', validate='m:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not hhs.jobacc30.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retail density\n",
    "\n",
    "For every tract, sum all jobs within the tract or within tracts within 1500 meters, and divide by the area of the buffer as calculated by land area only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lodes = pd.read_csv('../data/lodes/ca_wac_S000_JT00_2017.csv', dtype={'w_geocode': 'str'})\n",
    "lodes['tract'] = lodes.w_geocode.str.slice(0, 11)\n",
    "lodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tractlodes = pd.DataFrame(lodes.groupby('tract').CNS07.sum().rename('retailjobs')) # retail trade\n",
    "tractlodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all retail within 1500m of tract boundaries\n",
    "tracts = gp.read_file('../data/tracts/tl_2019_06_tract.shp')\n",
    "tracts = tracts[tracts.COUNTYFP.isin(['025', '037', '059', '065', '071', '111'])].to_crs(epsg=26943)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffered = tracts.copy()\n",
    "buffered['geometry'] = buffered.buffer(1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not buffered.GEOID.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = gp.sjoin(buffered, tracts, op='intersects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = neighbors.merge(tractlodes, left_on='GEOID_right', right_index=True, how='left', validate='m:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tractlocalretail = neighbors[['GEOID_left', 'retailjobs', 'ALAND_right']].groupby('GEOID_left').agg(np.sum)\n",
    "tractlocalretail['retailDensJobsSqKm'] = tractlocalretail.retailjobs / tractlocalretail.ALAND_right * 1e6\n",
    "tractlocalretail.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracthsg = pd.read_csv('../data/tract_housing/ACS_17_5YR_DP04_with_ann.csv', skiprows=[1], dtype={'GEO.id2': 'str'})\n",
    "tracthsgheader = pd.read_csv('../data/tract_housing/ACS_17_5YR_DP04_with_ann.csv', nrows=2, header=None).transpose()\n",
    "tracthsgheader.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tractlocalretail = tractlocalretail.merge(tracthsg[['GEO.id2', 'HC01_VC03']].rename(columns={'HC01_VC03': 'totalHousingUnits'}),\n",
    "                                         left_index=True, right_on='GEO.id2', how='left', validate='1:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the MABLE geographic crosswalk\n",
    "tractPumaXwalk = pd.read_csv('../data/mable_tracts_pumas.csv', skiprows=[1], dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tractPumaXwalk['tract_geoid'] = tractPumaXwalk.county.str.cat(tractPumaXwalk.tract.str.replace('.', ''))\n",
    "tractPumaXwalk['puma_geoid'] = tractPumaXwalk.state.str.cat(tractPumaXwalk.puma12)\n",
    "assert np.all(tractPumaXwalk.afact == '1 ') # all tracts should be completely in one puma\n",
    "tractPumaXwalk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tractlocalretail = tractlocalretail.merge(tractPumaXwalk[['tract_geoid', 'puma_geoid']], left_on='GEO.id2', right_on='tract_geoid',\n",
    "                                          how='left', validate='1:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tractlocalretail.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumalocalretail = pd.DataFrame(tractlocalretail.groupby('puma_geoid')\\\n",
    "                               .apply(lambda df: np.average(df.retailDensJobsSqKm, weights=df.totalHousingUnits))\\\n",
    "                               .rename('retailJobDensSqKm')).reset_index()\n",
    "pumalocalretail['puma'] = pumalocalretail.puma_geoid.str.slice(2)\n",
    "pumalocalretail.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs = hhs.reset_index().merge(pumalocalretail[['retailJobDensSqKm', 'puma']], on='puma', how='left', validate='m:1').set_index('serial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alts = hhs.groupby('choice', as_index=False).agg({\n",
    "    'fifthGradeMathMedianProficient': lambda x: x.iloc[0],\n",
    "    'bedrooms': np.mean,\n",
    "    'jobAccessAuto': lambda x: x.iloc[0],\n",
    "    'intersectionDens': lambda x: x.iloc[0],\n",
    "    'annvalue': np.median,\n",
    "    'sfmf': lambda x: x.iloc[0],\n",
    "    'retailJobDensSqKm': lambda x: x.iloc[0],\n",
    "    'jobacc30': lambda x: x.iloc[0]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor-analyze density variables\n",
    "\n",
    "To reduce collinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alts['puma'] = alts.choice.str.slice(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puma_dens = alts.groupby('puma')[['retailJobDensSqKm', 'jobacc30', 'jobAccessAuto', 'intersectionDens']].first()\n",
    "puma_dens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternately, could use a PCA to extract a single factor as this may be grasping at straws trying to differentiate these things. I also tried oblimin but correlation between the two factors was >.9. A third varimax factor is basically 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faz = FactorAnalyzer(rotation='varimax', n_factors=2)\n",
    "faz.fit(puma_dens)\n",
    "loadings = pd.DataFrame(faz.loadings_, index=puma_dens.columns, columns=['regacc', 'locacc'])\n",
    "# make sure they came out in the expected order and with expected sign\n",
    "assert loadings.loc['jobAccessAuto', 'regacc'] > 0\n",
    "assert loadings.loc['retailJobDensSqKm', 'locacc'] > 0\n",
    "assert loadings.loc['jobAccessAuto', 'regacc'] > loadings.loc['jobAccessAuto', 'locacc']\n",
    "loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_loadings = loadings.where(loadings > 0.3).round(2).fillna('').rename(\n",
    "    index={\n",
    "        'retailJobDensSqKm': 'Retail job density near PUMA',\n",
    "        'jobacc30': 'Access to jobs via transit within 30 minutes',\n",
    "        'jobAccessAuto': 'Access to jobs via auto within 45 minutes', # TODO confirm time\n",
    "        'intersectionDens': 'Intersection density'\n",
    "    },\n",
    "    columns={\n",
    "        'regacc': 'Regional access',\n",
    "        'locacc': 'Local access'\n",
    "    }\n",
    ")\n",
    "pretty_loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pretty_loadings.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puma_dens.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(faz.transform(puma_dens), index=puma_dens.index, columns=['regacc', 'locacc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(scores.regacc, scores.locacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alts = alts.merge(scores, left_on='puma', right_index=True, how='left', validate='m:1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commute to work\n",
    "\n",
    "TODO exclude out-of-state workers (data errors?) and telecommuters.\n",
    "\n",
    "After Tra (2007), just check if the PUMA overlaps the work PUMA - computing distances is too noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# every place of work puma is a collocation of regular PUMAs, and this file from https://usa.ipums.org/usa/volii/10pwpuma.shtml\n",
    "# maps them to each other\n",
    "powPumaComposition = pd.read_excel('../data/puma_migpuma1_pwpuma00.xls', skiprows=2).rename(columns={\n",
    "    'State of Residence (ST)': 'state', 'PUMA': 'puma',\n",
    "       'Place of Work State (PWSTATE2) or Migration State (MIGPLAC1)': 'powState',\n",
    "       'PWPUMA00 or MIGPUMA1': 'powPuma'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caPowPumaComposition = powPumaComposition.loc[powPumaComposition.state == '06', ['puma', 'powPuma']].copy().astype('int64')\n",
    "caPowPumaComposition['puma'] = caPowPumaComposition.puma.astype('str').str.zfill(5)\n",
    "caPowPumaComposition['powPuma'] = caPowPumaComposition.powPuma.astype('str').str.zfill(5)\n",
    "caPowPumaComposition.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipums['workpuma'] = ipums.workpuma.astype('str').str.zfill(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alts = alts.merge(caPowPumaComposition, on='puma', how='left', validate='m:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute relevant variables for all individuals\n",
    "# only count people who commute in person to workplaces in LA County, Orange County\n",
    "# Originally I was going to include surrounding counties too, but they would have the same effect on all alternatives so would drop out.\n",
    "# This is much too slow for the full sample. Compute on the fly in eqsormo (later).\n",
    "# commuters = ipums[ipums.worker & (ipums.tranwork != 'Worked at home') & (ipums.workstate == 'California') &\\\n",
    "#                     ipums.workcounty.isin([25, 37, 59, 65, 71, 111])].copy() # 37: LA county, 59: Orange County\n",
    "# commuters['forty2'] = 42\n",
    "# alts['forty2'] = 42\n",
    "# # cross join\n",
    "# jtw = commuters[['serial', 'workpuma', 'forty2']].merge(alts.reset_index(), on='forty2', how='left', validate='m:m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jtw['liveworksame'] = (jtw.workpuma == jtw.powPuma).astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hhalts = jtw.groupby(['serial', 'choice']).agg({\n",
    "#     'liveworksame': np.mean\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hhalts.liveworksame.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alts.set_index('choice', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex to match for non-worker households\n",
    "# hhalts = hhalts.reindex(pd.MultiIndex.from_product([hhs.index.values, alts.index.values]), fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hhalts.liveworksame.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split alternatives by owner and renter\n",
    "\n",
    "Pre-interact all terms so we can put them into the model without allowing additional levels of interaction in eqsormo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs['renter'] = hhs.ownershp == 'Renter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alts['intersectionDens'] /= 100\n",
    "alts['retailJobDensSqKm'] /= 100\n",
    "alts['jobAccessAuto'] /= 1e5\n",
    "alts['fifthGradeMathMedianProficient'] /= 100\n",
    "alts['jobacc30'] /= 1e5\n",
    "\n",
    "alts['sfh'] = alts.sfmf == 'SF'\n",
    "\n",
    "alts['hiregacc'] = alts.regacc >= np.percentile(scores.regacc, 90)\n",
    "alts['hilocacc'] = alts.locacc >= np.percentile(scores.locacc, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alts['annvalue'] /= 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs['hhincome'] /= 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alts = alts.set_index('choice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminate households\n",
    "hhs['annvalue'] = alts.annvalue.loc[hhs.choice].to_numpy()\n",
    "to_drop = (hhs.hhincome <= hhs.annvalue) | (hhs.hhincome < 15)\n",
    "print(to_drop.sum())\n",
    "hhs = hhs[~to_drop].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs.hhincome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs['inc_under_50k'] = (hhs.hhincome <  50)\n",
    "hhs['inc_50_100k'] = ((hhs.hhincome >= 50) & (hhs.hhincome < 100))\n",
    "hhs['inc_100k_plus'] = (hhs.hhincome >= 100)\n",
    "assert (hhs[['inc_under_50k', 'inc_50_100k', 'inc_100k_plus']].sum(axis=1) == 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs['inc_under_50k:immigrant'] = hhs.inc_under_50k & hhs.immigrant\n",
    "hhs['inc_50_100k:immigrant'] = hhs.inc_50_100k & hhs.immigrant\n",
    "hhs['inc_100k_plus:immigrant'] = hhs.inc_100k_plus & hhs.immigrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs['inc_under_50k:child:university'] = hhs.inc_under_50k & hhs.child & hhs.university\n",
    "hhs['inc_50_100k:child:university'] = hhs.inc_50_100k & hhs.child & hhs.university\n",
    "hhs['inc_100k_plus:child:university'] = hhs.inc_100k_plus & hhs.child & hhs.university\n",
    "\n",
    "hhs['inc_under_50k:child:not_university'] = hhs.inc_under_50k & hhs.child & ~hhs.university\n",
    "hhs['inc_50_100k:child:not_university'] = hhs.inc_50_100k & hhs.child & ~hhs.university\n",
    "hhs['inc_100k_plus:child:not_university'] = hhs.inc_100k_plus & hhs.child & ~hhs.university\n",
    "\n",
    "assert (\n",
    "    hhs[['inc_under_50k:child:university', 'inc_50_100k:child:university', 'inc_100k_plus:child:university',\n",
    "        'inc_under_50k:child:not_university', 'inc_50_100k:child:not_university', 'inc_100k_plus:child:not_university']]\n",
    "    .sum(axis=1) == hhs.child\n",
    ").all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs['hhwt'] /= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs['vehchoice'] = hhs.vehicles.replace({\n",
    "    '1 available': '1',\n",
    "    '4': '3+', \n",
    "    '3': '3+',\n",
    "    'No vehicles available': '0',\n",
    "    '5': '3+',\n",
    "    '6 (6+, 2000, ACS and PRCS)': '3+'\n",
    "})\n",
    "\n",
    "# put zero-vehicle households first, so that that is the reference category\n",
    "hhs = hhs.sort_values('vehchoice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alts['rent'] = pd.Series(alts.index, index=alts.index).str.endswith('rent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rent_own_cols = ['fifthGradeMathMedianProficient', 'bedrooms', 'jobAccessAuto',\n",
    "       'intersectionDens', 'retailJobDensSqKm', 'jobacc30',\n",
    "        'sfh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alts[rent_own_cols] = alts[rent_own_cols].astype('float64')\n",
    "alts['hilocacc'] = alts.hilocacc.astype('float64')\n",
    "alts['hiregacc'] = alts.hiregacc.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rentals = alts[alts.rent].rename(columns={c: f'{c}:rent' for c in rent_own_cols}).copy()\n",
    "# we want to preserve the non-interacted versions for the vehicle choice model\n",
    "rentals['locacc:rent'] = rentals.locacc\n",
    "rentals['hilocacc:rent'] = rentals.hilocacc\n",
    "rentals['regacc:rent'] = rentals.regacc\n",
    "rentals['hiregacc:rent'] = rentals.hiregacc\n",
    "ownoccs = alts[~alts.rent].rename(columns={c: f'{c}:own' for c in rent_own_cols}).copy()\n",
    "ownoccs['locacc:own'] = ownoccs.locacc\n",
    "ownoccs['hilocacc:own'] = ownoccs.hilocacc\n",
    "ownoccs['regacc:own'] = ownoccs.regacc\n",
    "ownoccs['hiregacc:own'] = ownoccs.hiregacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all nulls are from concat\n",
    "assert not alts.isnull().any().any()\n",
    "full_alts = pd.concat([rentals, ownoccs]).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_alts['hilocacc:rent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_alts.drop(columns=['sfmf']).to_parquet('../data/full_alts_fixed_acc.parquet')\n",
    "hhs.drop(columns=['appxage', 'sfmf']).to_parquet('../data/full_hh_fixed_acc.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_alts['sfh:rent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_alts.locacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs.numprec.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs.hhwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhs[['inc_under_50k', 'inc_50_100k', 'inc_100k_plus']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
