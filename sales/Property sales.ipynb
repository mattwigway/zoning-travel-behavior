{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rotary-twelve",
   "metadata": {},
   "source": [
    "# Property sales\n",
    "\n",
    "Given that a property is profitable to redevelop, what is the likelihood it actually will be? Build a logistic model of property sales and an auction model of developers and homeowners bidding on a property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats\n",
    "import geopandas as gp\n",
    "import libpysal.weights\n",
    "from glob import glob\n",
    "from census import Census\n",
    "from os import environ\n",
    "import matplotlib.patches as mpatch\n",
    "import textwrap\n",
    "\n",
    "DB_URI = 'postgresql://matthewc@localhost/matthewc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-university",
   "metadata": {},
   "outputs": [],
   "source": [
    "capi = Census(environ['CENSUS_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-peninsula",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_sales = pd.read_sql('''\n",
    "    WITH trans_hist AS (SELECT DISTINCT \"PropertyInfo_ImportParcelID\" \n",
    "        FROM diss.ztrans z\n",
    "        WHERE SUBSTRING(z.\"Main_RecordingDate\", 1, 4) IN ('2013', '2014', '2015', '2016', '2017'))\n",
    "    SELECT gid, county, clean_apn, building_yearbuilt, building_totalbedrooms, building_noofunits, building_totalcalculatedbathcount,\n",
    "    puma, ST_Area(geog) AS area_sqm, h.\"PropertyInfo_ImportParcelID\" IS NOT NULL AS sold, scag_zn_co, building_propertylandusestndcode\n",
    "    FROM diss.gp16 p\n",
    "        LEFT JOIN trans_hist h ON (p.Main_ImportParcelID = h.\"PropertyInfo_ImportParcelID\")\n",
    "        \n",
    "        WHERE p.scag_zn_co IN (\n",
    "            '1110', -- single family residential\n",
    "            '1111', -- high dens SF residential\n",
    "            '1112', -- med dens SF residential\n",
    "            '1113', -- low dens SF residential\n",
    "            '1150'  -- rural residential`\n",
    "        )\n",
    "        AND p.building_propertylandusestndcode = 'RR101'\n",
    "        AND NOT Main_ImportParcelID IS NULL\n",
    "        AND puma IS NOT NULL\n",
    "        \n",
    "''', DB_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-camping",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_sales.county.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-lighting",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_sales.loc[orig_sales.county == 'Imperial'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-floor",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_sales.loc[orig_sales.county == 'Imperial', ['building_totalbedrooms', 'building_totalcalculatedbathcount']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_len = len(orig_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_sales[(orig_sales.building_noofunits <= 1) | orig_sales.building_noofunits.isnull()].county.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = (\n",
    "    orig_sales[\n",
    "    ((orig_sales.building_noofunits <= 1) | orig_sales.building_noofunits.isnull()) &\n",
    "    # allow nan year builts in Imperial County, average will just get absorbed into fixed effects\n",
    "    # There is a single PUMA, 02500, that covers all of Imperial County and nothing else\n",
    "    (~orig_sales.building_yearbuilt.isnull() | (orig_sales.county == 'Imperial'))]\n",
    ".dropna(subset=['building_totalbedrooms', 'building_totalcalculatedbathcount'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sales) / orig_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (sales.loc[sales.building_yearbuilt.isnull(), 'county'] == 'Imperial').all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-missouri",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['building_yearbuilt'] = sales.building_yearbuilt.fillna(0) # will wind up in base category in model, PUMA fixed effect will account for average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.county.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-square",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sales.puma.value_counts().min() > 4 # avoid convergence problems with fixed effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['yrbltcat'] = pd.cut(sales.building_yearbuilt, [-np.inf, 1940, 1960, 1980, 2000, np.inf])\n",
    "sales['beds'] = np.minimum(sales.building_totalbedrooms, 4)\n",
    "exog = sales[['yrbltcat', 'beds', 'building_totalcalculatedbathcount', 'puma', 'area_sqm']]\n",
    "exog = sm.add_constant(pd.get_dummies(exog, columns=['yrbltcat', 'beds', 'puma']).drop(columns=['puma_07104', 'yrbltcat_(-inf, 1940.0]', 'beds_2']))\n",
    "mod = sm.Logit(sales.sold, exog)\n",
    "fit = mod.fit()\n",
    "fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_sale_prob = pd.Series(fit.predict(), index=sales.gid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-discipline",
   "metadata": {},
   "source": [
    "## Vacant property sale model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-monroe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vac = pd.read_sql('''\n",
    "    WITH trans_hist AS (SELECT DISTINCT \"PropertyInfo_ImportParcelID\" \n",
    "        FROM diss.ztrans z\n",
    "        WHERE SUBSTRING(z.\"Main_RecordingDate\", 1, 4) IN ('2013', '2014', '2015', '2016', '2017'))\n",
    "    SELECT gid, clean_apn, county, building_propertylandusestndcode\n",
    "    puma, ST_Area(geog) AS area_sqm, h.\"PropertyInfo_ImportParcelID\" IS NOT NULL AS sold\n",
    "    FROM diss.gp16 p\n",
    "        LEFT JOIN trans_hist h ON (p.Main_ImportParcelID = h.\"PropertyInfo_ImportParcelID\")\n",
    "        \n",
    "        WHERE p.scag_zn_co IN (\n",
    "            '1110', -- single family residential\n",
    "            '1111', -- high dens SF residential\n",
    "            '1112', -- med dens SF residential\n",
    "            '1113', -- low dens SF residential\n",
    "            '1150'  -- rural residential\n",
    "        )\n",
    "        AND p.building_propertylandusestndcode = 'VL101'\n",
    "        AND NOT Main_ImportParcelID IS NULL\n",
    "        AND puma IS NOT NULL\n",
    "''', DB_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "vac.county.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-dictionary",
   "metadata": {},
   "outputs": [],
   "source": [
    "puma_vcs = vac.puma.value_counts()\n",
    "# lower bar here than in sales since there are fewer properties than property sales\n",
    "pumas_to_merge = puma_vcs[puma_vcs < 5].index\n",
    "pumas_to_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-corps",
   "metadata": {},
   "outputs": [],
   "source": [
    "pumas = gp.read_file('/Volumes/Pheasant Ridge/IPUMS/pumas/socal_pumas_projected.shp').dissolve('PUMA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "queen_weights = libpysal.weights.Queen(pumas.geometry, ids=pumas.index.to_list()).to_adjlist().set_index('focal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with largest adjacent\n",
    "merges = {}\n",
    "for puma in pumas_to_merge:\n",
    "    candidates = queen_weights.loc[puma].neighbor\n",
    "    candidates = [i for i in candidates if i in puma_vcs.index]\n",
    "    if len(candidates) == 0:\n",
    "        print(f'no neighbors for {puma}!')\n",
    "    else:\n",
    "        neighbor = puma_vcs.loc[candidates].idxmax()\n",
    "        merges[puma] = neighbor\n",
    "# these don't end up merged into large enough pumas\n",
    "merges['03710'] = '03738'\n",
    "merges['03717'] = '03738'\n",
    "merges['03738'] = '03738'\n",
    "merges['03753'] = '03738'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-captain",
   "metadata": {},
   "outputs": [],
   "source": [
    "vac['merged_puma'] = vac.puma.replace(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-electric",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for perfect prediction\n",
    "pp = vac.groupby('merged_puma').sold.mean()\n",
    "pumas_to_merge = pp[(pp == 1) | (pp == 0)].index\n",
    "\n",
    "pp_merges = {}\n",
    "\n",
    "# find adjacent PUMAs without the same problem\n",
    "for  puma in pumas_to_merge:\n",
    "    candidates = queen_weights.loc[puma].neighbor\n",
    "    candidates = [i for i in candidates if i in puma_vcs.index and pp[merges[i] if i in merges else i] != pp[puma]]\n",
    "    if len(candidates) == 0:\n",
    "        print(f'no neighbors for {puma}!')\n",
    "    else:\n",
    "        neighbor = puma_vcs.loc[candidates].idxmax()\n",
    "        pp_merges[puma] = merges[neighbor] if neighbor in merges else neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-worry",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = {**merges, **pp_merges}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "vac['merged_puma'] = vac.puma.replace(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-sally",
   "metadata": {},
   "outputs": [],
   "source": [
    "vac.merged_puma.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-holder",
   "metadata": {},
   "outputs": [],
   "source": [
    "vac.groupby('merged_puma').sold.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-candy",
   "metadata": {},
   "outputs": [],
   "source": [
    "puma_vcs.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "exog = sm.add_constant(pd.get_dummies(vac[['area_sqm', 'merged_puma']], columns=['merged_puma']).drop(columns=['merged_puma_' + puma_vcs.idxmax()]))\n",
    "vacmod = sm.Logit(vac.sold, exog)\n",
    "vacfit = vacmod.fit()\n",
    "vacfit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "vac_sale_prob = pd.Series(vacfit.predict(), index=vac.gid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-translator",
   "metadata": {},
   "source": [
    "## Put probabilities together\n",
    "\n",
    "And fill in for any non-vacant properties missing probabilities (all vacant properties should have probabilities, no missing data issues)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-reverse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the number of properties in the profitability model\n",
    "assert len(vac) + len(orig_sales) == 2777552"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-strength",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_sales = orig_sales.set_index('gid')\n",
    "orig_sales['sale_prob'] = ex_sale_prob.reindex(orig_sales.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in null sales\n",
    "orig_sales['ersatz_sale_prob'] = orig_sales.groupby('puma').sale_prob.transform(lambda s: s.dropna().mean() if not np.all(pd.isnull(s)) else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_sale_prob = orig_sales.sale_prob.fillna(orig_sales.ersatz_sale_prob)\n",
    "assert not ex_sale_prob.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_prob = pd.concat([vac_sale_prob, ex_sale_prob])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(sale_prob) == 2777552"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-separate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigstars (p):\n",
    "    if p < 0.001:\n",
    "        return '***'\n",
    "    elif p < 0.01:\n",
    "        return '**'\n",
    "    elif p < 0.01:\n",
    "        return '*'\n",
    "    elif p < 0.1:\n",
    "        return '.'\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_table = pd.DataFrame({\n",
    "    'Odds ratio': np.exp(fit.params),\n",
    "    'Coef': fit.params,\n",
    "    'Std. err.': fit.bse,\n",
    "    't-value': fit.tvalues,\n",
    "    'p-value': fit.pvalues\n",
    "}).round(2)\n",
    "\n",
    "fit_table['Odds ratio'] = fit_table['Odds ratio'].astype(str).str.cat(fit.pvalues.apply(sigstars))\n",
    "\n",
    "fit_table = fit_table.loc[[i for i in fit_table.index if not i.startswith('puma_')]] # remove fixed effects\n",
    "fit_table.loc['Sample size', 'Odds ratio'] = fit.nobs\n",
    "fit_table.loc['Pseudo R^2', 'Odds ratio'] = round(fit.prsquared, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-blink",
   "metadata": {},
   "outputs": [],
   "source": [
    "vacfit_table = pd.DataFrame({\n",
    "    'Odds ratio': np.exp(vacfit.params),\n",
    "    'Coef': vacfit.params,\n",
    "    'Std. err.': vacfit.bse,\n",
    "    't-value': vacfit.tvalues,\n",
    "    'p-value': vacfit.pvalues\n",
    "}).round(2)\n",
    "\n",
    "vacfit_table['Odds ratio'] = vacfit_table['Odds ratio'].astype(str).str.cat(vacfit.pvalues.apply(sigstars))\n",
    "\n",
    "vacfit_table = vacfit_table.loc[[i for i in vacfit_table.index if not i.startswith('merged_puma_')]] # remove fixed effects\n",
    "vacfit_table.loc['Sample size', 'Odds ratio'] = vacfit.nobs\n",
    "vacfit_table.loc['Pseudo R^2', 'Odds ratio'] = round(vacfit.prsquared, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-electronics",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {\n",
    "    'const': 'Constant',\n",
    "    'building_totalcalculatedbathcount': 'Bathrooms',\n",
    "    'yrbltcat_(1940.0, 1960.0]': 'Built 1941–1960',\n",
    "    'yrbltcat_(1960.0, 1980.0]': 'Built 1961–1980',\n",
    "    'yrbltcat_(1980.0, 2000.0]': 'Built 1981–2000',\n",
    "    'yrbltcat_(2000.0, inf]': 'Built 2001–present',\n",
    "    'area_sqm': 'Lot area (square meters)',\n",
    "    'beds_0': 'No bedrooms',\n",
    "    'beds_1': 'One bedroom',\n",
    "    'beds_3': 'Three bedrooms',\n",
    "    'beds_4': 'Four or more bedrooms'\n",
    "}\n",
    "\n",
    "result_table = pd.concat([\n",
    "    fit_table.rename(index=names), vacfit_table.rename(index=names)],\n",
    "    keys=['Existing', 'Vacant']).fillna('')\n",
    "result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-concrete",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_table.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-repeat",
   "metadata": {},
   "source": [
    "## Map hedonic fixed effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot out just the rent new MF\n",
    "land = gp.read_file('../../sorting/data/ne_10m_land.shp').to_crs(epsg=26911)\n",
    "\n",
    "roads = pd.concat([gp.read_file(i).to_crs(epsg=26911) for i in glob('../../sorting/data/tl_roads/*.shp')], ignore_index=True)\n",
    "\n",
    "counties = gp.read_file('../../sorting/data/counties/tl_2019_us_county.shp').to_crs(26911)\n",
    "counties = counties[(counties.STATEFP == '06') & counties.NAME.isin(['Los Angeles', 'Ventura', 'Orange', 'Riverside', 'San Bernardino', 'Imperial'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "pumas[pumas.GEOID == '0611102']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-springer",
   "metadata": {},
   "outputs": [],
   "source": [
    "pumas['merged_puma'] = pumas.GEOID.replace(merges)\n",
    "\n",
    "f, axs = plt.subplots(2, 1, figsize=(9, 11))\n",
    "\n",
    "for lbl, model, ax in zip(['Existing homes', 'Vacant properties'], [fit, vacfit], axs):\n",
    "    fes = fit.params.loc[[i for i in fit.params.index if 'puma' in i]].rename('fe').reset_index()\n",
    "    fes['puma'] = '06' + fes['index'].str.slice(-5)\n",
    "    fes = fes.set_index('puma')\n",
    "    \n",
    "    # add back base effect\n",
    "    if lbl == 'Existing homes':\n",
    "        fes.loc['0607104'] = 0\n",
    "    elif lbl == 'Vacant properties':\n",
    "        fes.loc['06' + puma_vcs.idxmax()] = 0\n",
    "    else:\n",
    "        assert False, 'unexpected label'    \n",
    "    \n",
    "    pumas_fes = pumas.merge(fes, left_on='GEOID' if lbl == 'Existing homes' else 'merged_puma', right_index=True, validate='m:1')\n",
    "\n",
    "    pumas_fes.to_crs(epsg=26911).plot(ax=ax, column='fe', cmap='Blues', scheme='quantiles', legend=True)\n",
    "    roads.plot(color='#888888', ax=ax, lw=0.5)\n",
    "    counties.plot(edgecolor='#000',  facecolor='none', ax=ax, lw=1)\n",
    "    #water.plot(color='#aaaaaa', ax=ax)\n",
    "    ax.set_ylim(3.59e6, 3.95e6)\n",
    "    ax.set_xlim(2.74e5, 7.7e5)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(lbl)\n",
    "\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    #ax.set_axis_off()\n",
    "\n",
    "\n",
    "#     ax.legend(\n",
    "#         [mpatch.Patch(color=c) for c in colors.values()],\n",
    "#         [i.replace('$-', '-$').replace('$', '\\\\$') for i in colors.keys()],\n",
    "#         loc='lower left',\n",
    "#         title='Change in average rent',\n",
    "#         framealpha=1,\n",
    "#         fontsize='medium',\n",
    "#         title_fontsize='medium'\n",
    "#     )\n",
    "\n",
    "plt.savefig('../../dissertation/fig/sales/sale_fes.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-shanghai",
   "metadata": {},
   "source": [
    "### Standard deviation of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-mechanism",
   "metadata": {},
   "outputs": [],
   "source": [
    "rents = pd.read_sql('SELECT gid, total_rent, vacant_npv FROM diss.gp16', DB_URI).set_index('gid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-comparative",
   "metadata": {},
   "outputs": [],
   "source": [
    "rents[~rents.total_rent.isnull() & ~rents.vacant_npv.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-student",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_lnp = 0.3988261988782725\n",
    "discount_rate_ex = 0.04125\n",
    "cap_rate = 0.0479\n",
    "appreciation = 0.014\n",
    "op_cost = 0.45\n",
    "SCALE_FACTOR = 1.8031293436149882  # Scale factor to bring IPUMS rents in line with Zillow rental index\n",
    "VACANCY_RATE = 0.04  # 4% assumed vacancy\n",
    "TRANSACTION_COSTS = 0.09\n",
    "\n",
    "def get_sigma_npv (discount_rate_ex=discount_rate_ex, cap_rate=cap_rate, appreciation=appreciation, op_cost=op_cost, **kwargs):\n",
    "    lnp = np.log(rents.total_rent.dropna())\n",
    "    sigma_price = np.sqrt(np.exp(sigma_lnp**2 + 2 * lnp) * (np.exp(sigma_lnp**2) - 1)) # https://mathworld.wolfram.com/LogNormalDistribution.html\n",
    "    sigma_inc = 6.27 * sigma_price * SCALE_FACTOR\n",
    "    sigma_ex_npv = sigma_inc * (\n",
    "                    sum([1 / ((1 + discount_rate_ex)**i * (1 + appreciation) ** i) for i in range(10)])  # rental value\n",
    "            + 1 / cap_rate / (1 + discount_rate_ex) ** 10 * (1 + appreciation) ** 10 * (1 - TRANSACTION_COSTS) # ultimate sale value\n",
    "    ) \n",
    "\n",
    "    sigma_lnvp = 1.2619061577511022\n",
    "    # TODO dropping places where total_rent is not null b/c the ztrax and scag data don't completely agree on what is vacant\n",
    "    lnvp = np.log(rents.vacant_npv[rents.total_rent.isnull()].dropna())\n",
    "    sigma_vacnpv = np.sqrt(np.exp(sigma_lnvp ** 2 + 2 * lnvp) * np.exp(sigma_lnvp**2) - 1)\n",
    "\n",
    "    sigma_npv = pd.concat([sigma_ex_npv, sigma_vacnpv])\n",
    "    \n",
    "    return sigma_npv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-character",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_npv = get_sigma_npv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-michigan",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not sigma_npv.index.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-description",
   "metadata": {},
   "source": [
    "## Auction simulation\n",
    "\n",
    "Compute the probility that a bid for the intended use is the winning bid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-minutes",
   "metadata": {},
   "outputs": [],
   "source": [
    "npvs = pd.read_parquet('../data/Base_net_present_value.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-louis",
   "metadata": {},
   "outputs": [],
   "source": [
    "npvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-transsexual",
   "metadata": {},
   "outputs": [],
   "source": [
    "redevelopment_probs = [\n",
    "    (0.95, 1.0, 0.05), # up to 5% less than profitable: 5% redevelopment probability\n",
    "    (1, 1.1, 0.1), # 0-10%: 10%\n",
    "    (1.1, 1.25, 0.2),\n",
    "    (1.25, 1.5, 0.3),\n",
    "    (1.5, 2, 0.4),\n",
    "    (2, np.inf, 0.5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_auction (npvs, sale_prob):\n",
    "    joint_idx = npvs.index.intersection(sale_prob.index)\n",
    "\n",
    "    assert len(joint_idx) == 2777552 # make sure we're not missing anything\n",
    "\n",
    "    npvs = npvs.reindex(joint_idx) # why are there more in npv than sale_prob?\n",
    "    sale_prob = sale_prob.reindex(joint_idx)\n",
    "\n",
    "    ex_npv = npvs.existing\n",
    "    new_npvs = npvs[['sfh', 'duplex', 'threeplex', 'sixplex']]\n",
    "    max_new_npv = new_npvs.max(axis=1)\n",
    "    which_new_npv = new_npvs.idxmax(axis=1)\n",
    "    \n",
    "    redev_probs = pd.Series(np.zeros(len(max_new_npv)), index=max_new_npv.index)\n",
    "    \n",
    "    potential_profit = max_new_npv / ex_npv\n",
    "    \n",
    "    print(f'{(potential_profit > 1).sum()} lots profitable to redevelop')\n",
    "        \n",
    "    for lo, hi, prob in redevelopment_probs:\n",
    "        redev_probs.loc[(potential_profit >= lo) & (potential_profit < hi)] = prob\n",
    "        \n",
    "    combined_redev_prob = redev_probs * sale_prob\n",
    "    # splay it back out to all property types. only the most profitable and existing types will have any profitability\n",
    "    combined_prob = pd.DataFrame(np.zeros((len(npvs), len(npvs.columns)), np.float64), index=npvs.index, columns=npvs.columns)\n",
    "    print('computing combined probabilities')\n",
    "    for new_type in ['sfh', 'duplex', 'threeplex', 'sixplex']:\n",
    "        combined_prob.loc[which_new_npv == new_type, new_type] = combined_redev_prob.loc[which_new_npv == new_type]\n",
    "    combined_prob['existing'] = 1 - combined_redev_prob\n",
    "    \n",
    "    return combined_prob#, potential_profit#, redev_probs, max_new_npv, ex_npv, joint_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "puma_for_gid = pd.read_sql('SELECT gid, tract, puma, lu16, building_yearbuilt, building_propertylandusestndcode FROM diss.gp16', DB_URI).set_index('gid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-criterion",
   "metadata": {},
   "outputs": [],
   "source": [
    "hh = pd.read_parquet('../../sorting/data/full_hh.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-moldova",
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_to_puma = pd.read_csv('../../abm/data/2010_Census_Tract_to_2010_PUMA.csv', dtype='str')\n",
    "tract_to_puma['tract_geoid'] = tract_to_puma.STATEFP.str.cat(tract_to_puma.COUNTYFP).str.cat(tract_to_puma.TRACTCE)\n",
    "tract_to_puma['puma_geoid'] = tract_to_puma.STATEFP.str.cat(tract_to_puma.PUMA5CE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-journalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAP_RATE = 0.04424375\n",
    "discount_rate_ex = 0.06813142841167415\n",
    "\n",
    "scenarios = {\n",
    "    'Current appreciation':  {\n",
    "        'discount_rate_new': 0.11,\n",
    "        'discount_rate_ex': discount_rate_ex,\n",
    "        'cap_rate': CAP_RATE,\n",
    "        'appreciation': 0.0480328437009123,\n",
    "        'op_cost': 0.45,\n",
    "        'cost_scale': 1.1\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "scenarios['Base'] = {\n",
    "    'discount_rate_new': 0.11,\n",
    "    'discount_rate_ex': discount_rate_ex,\n",
    "    'cap_rate': CAP_RATE,\n",
    "    'appreciation': 0.014,  # CPI\n",
    "    'op_cost': 0.45,\n",
    "    'cost_scale': 1.1 # 10% contingency\n",
    "}\n",
    "\n",
    "scenarios['Low discount rate'] = {\n",
    "    'discount_rate_new': 0.08,\n",
    "    'discount_rate_ex': 0.04125,\n",
    "    'cap_rate': CAP_RATE,\n",
    "    'appreciation': 0.014,  # CPI\n",
    "    'op_cost': 0.45,\n",
    "    'cost_scale': 1.1 # 10% contingency\n",
    "}\n",
    "\n",
    "scenarios['Equal discount rate (8% existing and new)'] = {\n",
    "    'discount_rate_new': 0.08,\n",
    "    'discount_rate_ex': 0.08,\n",
    "    'cap_rate': CAP_RATE,\n",
    "    'appreciation': 0.014,  # CPI\n",
    "    'op_cost': 0.45,\n",
    "    'cost_scale': 1.1 # 10% contingency\n",
    "}\n",
    "\n",
    "scenarios['Low operating cost (25%)'] = {\n",
    "    'discount_rate_new': 0.11,\n",
    "    'discount_rate_ex': discount_rate_ex,\n",
    "    'cap_rate': CAP_RATE,\n",
    "    'appreciation': 0.0480328437009123,  # CPI\n",
    "    'op_cost': 0.25,\n",
    "    'cost_scale': 1.1 # 10% contingency\n",
    "}\n",
    "\n",
    "scenarios['High construction cost'] = {\n",
    "    'discount_rate_new': 0.11,\n",
    "    'discount_rate_ex': discount_rate_ex,\n",
    "    'cap_rate': CAP_RATE,\n",
    "    'appreciation': 0.014,  # CPI\n",
    "    'op_cost': 0.45,\n",
    "    'cost_scale': 1.4 # 30% on top\n",
    "}\n",
    "\n",
    "marginal_units = {}\n",
    "scenario_units = {}\n",
    "construction_and_demolition = {}\n",
    "tract_marginal_units = {}\n",
    "tract_units = {}\n",
    "\n",
    "short_names = {\n",
    "    'Base': 'npv_base',\n",
    "    'Current appreciation': 'npv_current_appreciation',\n",
    "    'Equal discount rate (8% existing and new)': 'npv_eq_discount',\n",
    "    'Low discount rate': 'npv_low_discount',\n",
    "    'Low operating cost (25%)': 'npv_low_opcost',\n",
    "    'High construction cost': 'high_const_cost'\n",
    "}\n",
    "\n",
    "for sname, short_name in list(short_names.items()):\n",
    "    short_names[f'{sname}_hqta'] = f'{short_name}_hqta'\n",
    "    scenarios[f'{sname}_hqta'] = scenarios[sname]\n",
    "\n",
    "# get combined probs for all scenarios\n",
    "for scenario in sorted(glob('../data/*_net_present_value.parquet')):\n",
    "    sname = scenario[8:-26]\n",
    "    print(sname)\n",
    "    \n",
    "    sc_npvs = pd.read_parquet(scenario)\n",
    "    sc_probs = simulate_auction(sc_npvs[['existing', 'sfh', 'duplex', 'threeplex', 'sixplex']], sale_prob)\n",
    "    sc_probs.to_parquet(f'../data/{sname}_redevelopment_probability.parquet')\n",
    "    sc_buildings = sc_probs.sum()\n",
    "    \n",
    "    sc_probs_geo = sc_probs.join(puma_for_gid.reindex(sc_probs.index))\n",
    "    teardowns = sc_probs_geo[sc_probs_geo.building_propertylandusestndcode == 'RR101'].copy()\n",
    "    \n",
    "    # Figure out marginal units\n",
    "    new_units = sc_buildings.sfh + 2 * sc_buildings.duplex + 3 * sc_buildings.threeplex + 6 * sc_buildings.sixplex\n",
    "    destroyed_units = (1 - teardowns.existing).sum()\n",
    "        \n",
    "    margun = new_units - destroyed_units\n",
    "    marginal_units[sname] = margun\n",
    "    print(f'new units: {new_units} destroyed units: {destroyed_units} marginal units: {margun}')\n",
    "    \n",
    "    # prepare scenario for sorting\n",
    "    # how many new multifamily homes are being created in each PUMA\n",
    "    # Do also for tracts - used in population synthesis\n",
    "    for grp in ('puma', 'tract'):\n",
    "        grpd = sc_probs_geo.groupby(grp)\n",
    "        mfh_new = grpd.sixplex.sum() * 6 + grpd.threeplex.sum() * 3 + grpd.duplex.sum() * 2\n",
    "        sfh_new = grpd.sfh.sum()\n",
    "\n",
    "        # and how many are torn down?\n",
    "        assert (sc_probs_geo.building_propertylandusestndcode == 'VL101').any() # make sure we didn't lose vacant properties\n",
    "        teardowns = sc_probs_geo[sc_probs_geo.building_propertylandusestndcode == 'RR101'].copy()\n",
    "\n",
    "        teardowns['teardown_prob'] = 1 - teardowns.existing\n",
    "        teardowns['built_2000_or_later'] = (teardowns.building_yearbuilt >= 2000).fillna(False) # assume missing is old\n",
    "\n",
    "        sfh_new -= teardowns[teardowns.built_2000_or_later].groupby(grp).teardown_prob.sum()\n",
    "        sfh_old = -teardowns[~teardowns.built_2000_or_later].groupby(grp).teardown_prob.sum()\n",
    "\n",
    "        weighted_supply = hh.groupby('choice').hhwt.sum() / 100\n",
    "\n",
    "        weighted_supply = weighted_supply.reset_index()\n",
    "\n",
    "        weighted_supply[['puma', 'sfmf', 'age', 'tenure']] = weighted_supply.choice.str.split('_', expand=True)\n",
    "\n",
    "        rent_prop = weighted_supply.groupby(['puma', 'sfmf', 'age']).apply(lambda df: df.loc[df.tenure == 'rent', 'hhwt'].sum() / df.hhwt.sum()).rename('rent_prop')\n",
    "\n",
    "        if grp == 'puma':\n",
    "            rent_prop.index = [f'{puma}_{sfmf}_{age}' for puma, sfmf, age in rent_prop.index]\n",
    "        elif grp == 'tract':\n",
    "            # use puma level rent/own proportion, broadcast to tract level\n",
    "            rent_prop = rent_prop.reset_index()\n",
    "            rent_prop = rent_prop.merge(tract_to_puma, left_on='puma', right_on='PUMA5CE', how='inner', validate='m:m')\n",
    "            \n",
    "            # deal with one Census tract in Woodland Hills that is split between PUMAs using in those\n",
    "            # two pumas put together\n",
    "            wdhls = (\n",
    "                weighted_supply[weighted_supply.puma.isin(['03725', '03726'])].groupby(['sfmf', 'age'])\n",
    "                .apply(lambda df: df.loc[df.tenure == 'rent', 'hhwt'].sum() / df.hhwt.sum()).rename('rent_prop')\n",
    "            ).reset_index()\n",
    "            \n",
    "            wdhls['tract_geoid'] = '06037137000'\n",
    "            \n",
    "            rent_prop = pd.concat([rent_prop, wdhls], ignore_index=True)\n",
    "            rent_prop = rent_prop.set_index(['tract_geoid', 'sfmf', 'age'])\n",
    "            rent_prop.index = [f'{tract}_{sfmf}_{age}' for tract, sfmf, age in rent_prop.index]\n",
    "            rent_prop = rent_prop.rent_prop  # peel back to just a multiindexed series\n",
    "            \n",
    "        sorting_inputs = pd.DataFrame({\n",
    "            'MF_new': mfh_new,\n",
    "            'MF_old': 0,\n",
    "            'SF_new': sfh_new,\n",
    "            'SF_old': sfh_old\n",
    "        })\n",
    "\n",
    "        sorting_inputs = sorting_inputs.stack()\n",
    "\n",
    "        sorting_inputs.index = [f'{g}_{htype}' for g, htype in sorting_inputs.index]\n",
    "        sorting_inputs\n",
    "\n",
    "        rent_prop = rent_prop.reindex(sorting_inputs.index)\n",
    "        assert not rent_prop.isnull().any()\n",
    "\n",
    "        renter_sorting_inputs = sorting_inputs * rent_prop\n",
    "        renter_sorting_inputs.index = [f'{i}_rent' for i in renter_sorting_inputs.index]\n",
    "\n",
    "        owner_sorting_inputs = sorting_inputs * (1 - rent_prop)\n",
    "        owner_sorting_inputs.index = [f'{i}_own' for i in owner_sorting_inputs.index]\n",
    "\n",
    "        sorting_inputs = pd.concat([renter_sorting_inputs, owner_sorting_inputs])\n",
    "\n",
    "        if grp == 'puma':\n",
    "            scenario_units[short_names[sname]] = sorting_inputs\n",
    "            sc_buildings.loc['Teardowns'] = teardowns.teardown_prob.sum()\n",
    "            construction_and_demolition[sname] = sc_buildings\n",
    "        else:\n",
    "            tract_units[short_names[sname]] = sorting_inputs\n",
    "    \n",
    "    \n",
    "    grpd_new = sc_probs_geo.groupby('tract')\n",
    "    tract_marginal_units[sname] = (\n",
    "        grpd_new.sfh.sum() + grpd_new.duplex.sum() * 2 + grpd_new.threeplex.sum() * 3 + grpd_new.sixplex.sum() * 6\n",
    "        - teardowns.groupby('tract').teardown_prob.sum()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "construction_and_demolition = pd.DataFrame(construction_and_demolition).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-longer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read profitability table from earlier chapter for comparison\n",
    "profitability_table = pd.read_parquet('../data/profitability_table.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-brother",
   "metadata": {},
   "outputs": [],
   "source": [
    "profitability_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-merit",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_table = construction_and_demolition.copy()\n",
    "# need to do this first before converting rest to number of units\n",
    "paper_table['ex_pct'] = paper_table.existing / (paper_table.existing + paper_table.sfh + paper_table.duplex + paper_table.threeplex + paper_table.sixplex) * 100\n",
    "paper_table['duplex'] *= 2\n",
    "paper_table['threeplex'] *= 3\n",
    "paper_table['sixplex'] *= 6\n",
    "paper_table['total_new'] = paper_table.sfh + paper_table.duplex + paper_table.threeplex + paper_table.sixplex\n",
    "paper_table['marginal_units'] = paper_table.total_new - paper_table.Teardowns\n",
    "paper_table.index = [i.replace('_hqta', ' (HQTA)') for i in paper_table.index]\n",
    "paper_table = paper_table.reindex(profitability_table.index)\n",
    "paper_table[['sfh', 'duplex', 'threeplex', 'sixplex', 'total_new', 'Teardowns', 'marginal_units']] /= 1000\n",
    "paper_table['marginal_units_orig'] = profitability_table['Marginal units'] / 1000\n",
    "paper_table[['sfh', 'duplex', 'threeplex', 'sixplex', 'total_new', 'marginal_units', 'Teardowns', 'marginal_units_orig']] =\\\n",
    "    paper_table[['sfh', 'duplex', 'threeplex', 'sixplex', 'total_new', 'marginal_units', 'Teardowns', 'marginal_units_orig']].round().astype('int64').apply(lambda c: c.apply('{:,d}'.format))\n",
    "paper_table['ex_pct'] = paper_table.ex_pct.apply('{:.1f}%'.format)\n",
    "paper_table = paper_table[['ex_pct', 'sfh', 'duplex', 'threeplex', 'sixplex', 'total_new', 'Teardowns', 'marginal_units', 'marginal_units_orig']].rename(columns={\n",
    "    'sfh': 'Single-family home',\n",
    "    'duplex': 'Duplex',\n",
    "    'threeplex': 'Threeplex',\n",
    "    'sixplex': 'Sixplex',\n",
    "    'ex_pct': 'Non-redeveloped parcels',\n",
    "    'total_new': 'Total',\n",
    "    'marginal_units': 'Marginal units',\n",
    "    'marginal_units_orig': 'Most profitable marginal units'\n",
    "})\n",
    "paper_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-tutorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paper_table.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_output = pd.DataFrame(scenario_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-finland",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_output.to_parquet('../data/npv_scenarios.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-recall",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_output.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-dutch",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-organic",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_output.npv_base[scenario_output.npv_base > 0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_units = pd.DataFrame(tract_units)\n",
    "tract_units.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-cabin",
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_units.to_parquet('../data/npv_tract_scenarios.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "mun = pd.Series(marginal_units).sort_values(ascending=False).rename(index={\n",
    "    'Current (8.3%) Appreciation': 'Current appreciation',\n",
    "       'Low discount rate (2.5% existing, 5% new)': 'Low discount rate',\n",
    "       'Equal discount rate (8% existing and new)': 'Equal existing/new\\ndiscount rate',\n",
    "       'Low operating cost (25%)': 'Low operating cost',\n",
    "       'High discount rate (6% existing, 12% new)': 'High discount rate'\n",
    "})\n",
    "plt.barh(np.arange(len(mun)), mun / 1_000_000, color='#8c1d40')\n",
    "plt.yticks(np.arange(len(mun)), mun.index)\n",
    "plt.xlabel('Number of new units (millions)')\n",
    "plt.savefig('uncertainty.svg')\n",
    "plt.savefig('uncertainty.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
